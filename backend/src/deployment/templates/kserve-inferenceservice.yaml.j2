apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: {{ deployment_id }}
  namespace: {{ namespace }}
  labels:
    app: llm-inference
    model: {{ model_id | replace('/', '-') }}
    deployment-id: {{ deployment_id }}
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
    description: "{{ use_case }} deployment - {{ reasoning | truncate(100) }}"
    {% if simulator_mode %}
    ai-assistant/simulator-mode: "true"
    {% endif %}
spec:
  predictor:
    minReplicas: {{ min_replicas }}
    maxReplicas: {{ max_replicas }}
    containers:
    - name: kserve-container
      {% if simulator_mode %}
      # Simulator mode: Use vLLM simulator (no GPU required)
      image: vllm-simulator:latest
      imagePullPolicy: Never  # Use local image from KIND
      env:
      - name: MODEL_NAME
        value: "{{ model_id }}"
      - name: GPU_TYPE
        value: "{{ gpu_type }}"
      - name: TENSOR_PARALLEL_SIZE
        value: "{{ tensor_parallel }}"
      - name: PORT
        value: "8080"
      resources:
        requests:
          cpu: "500m"
          memory: "512Mi"
        limits:
          cpu: "1"
          memory: "1Gi"
      {% else %}
      # Production mode: Use real vLLM with GPUs
      image: vllm/vllm-openai:{{ vllm_version }}
      args:
      - --model={{ model_id }}
      - --tensor-parallel-size={{ tensor_parallel }}
      - --gpu-memory-utilization={{ gpu_memory_utilization }}
      - --max-model-len={{ max_model_len }}
      - --dtype={{ dtype }}
      - --trust-remote-code
      - --port=8080
      env:
      - name: HUGGING_FACE_HUB_TOKEN
        valueFrom:
          secretKeyRef:
            name: hf-token-secret
            key: token
            optional: true
      resources:
        requests:
          cpu: "{{ cpu_request }}"
          memory: "{{ memory_request }}"
          nvidia.com/gpu: "{{ gpus_per_replica }}"
        limits:
          cpu: "{{ cpu_limit }}"
          memory: "{{ memory_limit }}"
          nvidia.com/gpu: "{{ gpus_per_replica }}"
      {% endif %}
      ports:
      - containerPort: 8080
        protocol: TCP
        name: http
      livenessProbe:
        httpGet:
          path: /health
          port: 8080
        {% if simulator_mode %}
        initialDelaySeconds: 10
        {% else %}
        initialDelaySeconds: 120
        {% endif %}
        periodSeconds: 30
        timeoutSeconds: 10
      readinessProbe:
        httpGet:
          path: /health
          port: 8080
        {% if simulator_mode %}
        initialDelaySeconds: 5
        {% else %}
        initialDelaySeconds: 60
        {% endif %}
        periodSeconds: 10
        timeoutSeconds: 5
    {% if not simulator_mode %}
    nodeSelector:
      nvidia.com/gpu.product: {{ gpu_type }}
    tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule
    {% endif %}
